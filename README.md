# Covid-19-DataPipeline
A Data Engineering Project that extracts data from Covid API and manipulates it to store and use for visualisations.

Project Description: Enhancing Data Engineering with REST API Integration and Efficient Extraction Using Python and AWS

Overview:
This data engineering project involves the utilization of a REST API to extract data, transform it, and load it into a SQL database via Dataspell. The Python script incorporates a task scheduler and cron job on AWS, optimizing the extraction of fresh data for each country by checking for changes in the API's dictionary. This efficient approach minimizes script execution time. The project also includes visualization of results using Matplotlib and Seaborn.

Project Goals:
1. Implement a comprehensive data engineering pipeline for extracting, transforming, and loading data using Python.
2. Utilize a REST API to retrieve data efficiently.
3. Integrate Dataspell for seamless loading into a SQL database.
4. Optimize data extraction by using a task scheduler and cron job on AWS.
5. Ensure the script efficiently extracts fresh data for each country.
6. Visualize the results using Matplotlib and Seaborn.

Technologies Used:
- Python
- AWS (Task Scheduler, Cron Job)
- REST API
- Dataspell
- Matplotlib
- Seaborn

Key Features:
1. REST API Integration:
   - Incorporate a REST API for data extraction, ensuring up-to-date information.

2. Dataspell Integration:
   - Utilize Dataspell for efficient loading of data into a SQL database.

3. Task Scheduler and Cron Job Optimization:
   - Implement AWS task scheduler and cron job for scheduled and optimized data extraction.

4. Efficient Country-specific Data Extraction:
   - Optimize the script to extract fresh data for each country, enhancing efficiency.

5. Visualization with Matplotlib and Seaborn:
   - Visualize the extracted and transformed data using Matplotlib and Seaborn for clear insights.

Project Workflow:
1. Trigger data extraction from the REST API using Python.
2. Transform the extracted data as required.
3. Load the processed data into a SQL database using Dataspell.
4. Schedule and optimize the script execution using AWS task scheduler and cron job.
5. Visualize the results using Matplotlib and Seaborn.

Skills Demonstrated:
- Integration of REST API for data retrieval.
- Efficient data loading into a SQL database using Dataspell.
- Optimization of data extraction using AWS task scheduler and cron job.
- Python scripting for data transformation and manipulation.
- Visualization of results using Matplotlib and Seaborn.
- Effective use of cloud services for task scheduling.

This project showcases proficiency in integrating various technologies to create an efficient and automated data engineering pipeline. The use of AWS, Dataspell, and Python demonstrates the ability to handle complex tasks in data extraction, transformation, and loading, providing valuable insights for analysis and decision-making.
